{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/maksym/research/berga_clust/da-corpora\"\n",
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl-Europarl.en-et.docs.test-cl\n",
      "cl-EMEA.en-et.docs.test-cl\n",
      "cl-Europarl.en-et.docs.dev-cl\n",
      "cl-JRC-Acquis.en-et.docs.dev-cl\n",
      "cl-EMEA.en-et.docs.dev-cl\n",
      "cl-OpenSubtitles.en-et.docs.dev-cl\n",
      "cl-OpenSubtitles.en-et.docs.test-cl\n",
      "cl-JRC-Acquis.en-et.docs.test-cl\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for file in glob.glob(\"*-cl\"):\n",
    "    print(file)\n",
    "    files.append(open(file, 'r').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce files to eglish sentences but keep doc ids\n",
    "\n",
    "sent_index = deepcopy(files)\n",
    "\n",
    "for ind, f in enumerate(files):\n",
    "    for i in range(len(f)):\n",
    "        sent_index[ind][i] = f[i].split('\\t')[0]\n",
    "        f[i] = f[i].split('\\t')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('xlm-roberta-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_value_of_dtype(dtype: torch.dtype):\n",
    "    if not dtype.is_floating_point:\n",
    "        raise TypeError(\"Only supports floating point dtypes.\")\n",
    "    if dtype == torch.float or dtype == torch.double:\n",
    "        return 1e-13\n",
    "    elif dtype == torch.half:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        raise TypeError(\"Does not support dtype \" + str(dtype))\n",
    "        \n",
    "def masked_mean(\n",
    "    vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    replaced_vector = vector.masked_fill(~mask, 0.0)\n",
    "\n",
    "    value_sum = torch.sum(replaced_vector, dim=dim, keepdim=keepdim)\n",
    "    value_count = torch.sum(mask, dim=dim, keepdim=keepdim)\n",
    "    return value_sum / value_count.float().clamp(min=tiny_value_of_dtype(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "    def __init__(self, model, tokeinzer):    \n",
    "        self.model = model.cuda()\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def embed_batch(self, batch):\n",
    "        batch = self.tokenizer.batch_encode_plus(batch, \n",
    "                          return_tensors='pt', \n",
    "                          truncation=True, \n",
    "                          padding=True, \n",
    "                          max_length=100)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            res = self.model(\n",
    "                input_ids=batch.input_ids.cuda(),\n",
    "                attention_mask=batch.attention_mask.cuda(),\n",
    "                output_hidden_states=True, \n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            hiddens = res['hidden_states'][7].cpu().detach()\n",
    "        \n",
    "        hiddens_sent = masked_mean(vector=hiddens, mask=batch.attention_mask.unsqueeze(2).bool(), dim=1)\n",
    "        return hiddens_sent.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4\n",
      "1 / 4\n",
      "2 / 4\n",
      "3 / 4\n",
      "0 / 4\n",
      "1 / 4\n",
      "2 / 4\n",
      "3 / 4\n",
      "0 / 4\n",
      "1 / 4\n",
      "2 / 4\n",
      "3 / 4\n",
      "0 / 4\n",
      "1 / 4\n",
      "2 / 4\n",
      "3 / 4\n",
      "0 / 4\n",
      "1 / 4\n",
      "2 / 4\n",
      "3 / 4\n",
      "0 / 4\n",
      "1 / 4\n",
      "2 / 4\n",
      "3 / 4\n",
      "0 / 4\n",
      "1 / 4\n",
      "2 / 4\n",
      "3 / 4\n",
      "0 / 4\n",
      "1 / 4\n",
      "2 / 4\n",
      "3 / 4\n",
      "CPU times: user 59.4 s, sys: 21.4 s, total: 1min 20s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "files_emb = []\n",
    "for sentences in files:\n",
    "\n",
    "    sent_emb = []\n",
    "    \n",
    "    bs = 1000\n",
    "    chunks = [sentences[x:x+bs] for x in range(0, len(sentences), bs)]\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"{i} / {len(chunks)}\") \n",
    "        chunk_emb = embedder.embed_batch(chunk)\n",
    "        sent_emb.extend(chunk_emb)\n",
    "    \n",
    "    files_emb.append(sent_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_sent_4 = pickle.load(open(\"kmeans_sent_4.pkl\", 'rb'))\n",
    "kmeans_sent_16 = pickle.load(open(\"kmeans_sent_16.pkl\", 'rb'))\n",
    "kmeans_sent_64 = pickle.load(open(\"kmeans_sent_64.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = []\n",
    "for file in glob.glob(\"*-cl\"):\n",
    "    fnames.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cl-Europarl.en-et.docs.test-cl',\n",
       " 'cl-EMEA.en-et.docs.test-cl',\n",
       " 'cl-Europarl.en-et.docs.dev-cl',\n",
       " 'cl-JRC-Acquis.en-et.docs.dev-cl',\n",
       " 'cl-EMEA.en-et.docs.dev-cl',\n",
       " 'cl-OpenSubtitles.en-et.docs.dev-cl',\n",
       " 'cl-OpenSubtitles.en-et.docs.test-cl',\n",
       " 'cl-JRC-Acquis.en-et.docs.test-cl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fe, fn in zip(files_emb, fnames):\n",
    "#     clusters_sent_4 = kmeans_sent_4.predict(fe)\n",
    "#     clusters_sent_16 = kmeans_sent_16.predict(fe)\n",
    "#     clusters_sent_64 = kmeans_sent_64.predict(fe)\n",
    "    \n",
    "    with open(f'{fn}.clusters_sent_4.txt', 'w') as f:\n",
    "        for item in kmeans_sent_4.predict(fe):\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(f'{fn}.clusters_sent_16.txt', 'w') as f:\n",
    "        for item in kmeans_sent_16.predict(fe):\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(f'{fn}.clusters_sent_64.txt', 'w') as f:\n",
    "        for item in kmeans_sent_64.predict(fe):\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_doc_4 = pickle.load(open(\"kmeans_doc_4.pkl\", 'rb'))\n",
    "kmeans_doc_16 = pickle.load(open(\"kmeans_doc_16.pkl\", 'rb'))\n",
    "kmeans_doc_64 = pickle.load(open(\"kmeans_doc_64.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = []\n",
    "for file in glob.glob(\"*-cl\"):\n",
    "    fnames.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cl-Europarl.en-et.docs.test-cl',\n",
       " 'cl-EMEA.en-et.docs.test-cl',\n",
       " 'cl-Europarl.en-et.docs.dev-cl',\n",
       " 'cl-JRC-Acquis.en-et.docs.dev-cl',\n",
       " 'cl-EMEA.en-et.docs.dev-cl',\n",
       " 'cl-OpenSubtitles.en-et.docs.dev-cl',\n",
       " 'cl-OpenSubtitles.en-et.docs.test-cl',\n",
       " 'cl-JRC-Acquis.en-et.docs.test-cl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2embs_list = []\n",
    "\n",
    "for sent_ids, sent_embs in zip(sent_index, files_emb):\n",
    "    doc2embs = defaultdict(list)\n",
    "    for i, doc_id in enumerate(sent_ids):\n",
    "        doc2embs[doc_id].append(sent_embs[i])\n",
    "    doc2embs_list.append(doc2embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d2embs in doc2embs_list:\n",
    "    for doc_id, embs in d2embs.items():\n",
    "        d2embs[doc_id] = np.mean(embs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_emb = []\n",
    "for i, f in enumerate(sent_index):\n",
    "    fe = []\n",
    "    for doc_id in f:\n",
    "        fe.append(doc2embs_list[i][doc_id])\n",
    "    files_emb.append(fe)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fe, fn in zip(files_emb, fnames):\n",
    "#     clusters_sent_4 = kmeans_sent_4.predict(fe)\n",
    "#     clusters_sent_16 = kmeans_sent_16.predict(fe)\n",
    "#     clusters_sent_64 = kmeans_sent_64.predict(fe)\n",
    "    \n",
    "    with open(f'{fn}.clusters_doc_4.txt', 'w') as f:\n",
    "        for item in kmeans_doc_4.predict(fe):\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(f'{fn}.clusters_doc_16.txt', 'w') as f:\n",
    "        for item in kmeans_doc_16.predict(fe):\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(f'{fn}.clusters_doc_64.txt', 'w') as f:\n",
    "        for item in kmeans_doc_64.predict(fe):\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl-Europarl.en-et.docs.test-cl\n",
      "[(1, 3107)]\n",
      "cl-EMEA.en-et.docs.test-cl\n",
      "[(1, 2573), (2, 683), (3, 59)]\n",
      "cl-Europarl.en-et.docs.dev-cl\n",
      "[(1, 3716)]\n",
      "cl-JRC-Acquis.en-et.docs.dev-cl\n",
      "[(0, 84), (1, 207), (2, 2714)]\n",
      "cl-EMEA.en-et.docs.dev-cl\n",
      "[(0, 12), (1, 1710), (2, 1618), (3, 8)]\n",
      "cl-OpenSubtitles.en-et.docs.dev-cl\n",
      "[(3, 3044)]\n",
      "cl-OpenSubtitles.en-et.docs.test-cl\n",
      "[(3, 3085)]\n",
      "cl-JRC-Acquis.en-et.docs.test-cl\n",
      "[(0, 15), (1, 476), (2, 2699)]\n"
     ]
    }
   ],
   "source": [
    "for fe, fn in zip(files_emb, fnames):\n",
    "#     clusters_sent_4 = kmeans_sent_4.predict(fe)\n",
    "#     clusters_sent_16 = kmeans_sent_16.predict(fe)\n",
    "#     clusters_sent_64 = kmeans_sent_64.predict(fe)\n",
    "    \n",
    "    print(fn)\n",
    "    print(sorted(Counter(kmeans_doc_4.predict(fe)).items()))\n",
    "    \n",
    "#     with open(f'{fn}.clusters_doc_16.txt', 'w') as f:\n",
    "#         for item in kmeans_doc_16.predict(fe):\n",
    "#             f.write(\"%s\\n\" % item)\n",
    "    \n",
    "#     with open(f'{fn}.clusters_doc_64.txt', 'w') as f:\n",
    "#         for item in kmeans_doc_64.predict(fe):\n",
    "#             f.write(\"%s\\n\" % item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl-Europarl.en-et.docs.test-cl\n",
      "[(0, 11), (1, 9), (2, 3066), (3, 21)]\n",
      "cl-EMEA.en-et.docs.test-cl\n",
      "[(0, 683), (1, 2532), (2, 46), (3, 54)]\n",
      "cl-Europarl.en-et.docs.dev-cl\n",
      "[(2, 3716)]\n",
      "cl-JRC-Acquis.en-et.docs.dev-cl\n",
      "[(0, 1506), (1, 168), (2, 1326), (3, 5)]\n",
      "cl-EMEA.en-et.docs.dev-cl\n",
      "[(0, 1616), (1, 1592), (2, 120), (3, 20)]\n",
      "cl-OpenSubtitles.en-et.docs.dev-cl\n",
      "[(3, 3044)]\n",
      "cl-OpenSubtitles.en-et.docs.test-cl\n",
      "[(3, 3085)]\n",
      "cl-JRC-Acquis.en-et.docs.test-cl\n",
      "[(0, 924), (1, 58), (2, 2208)]\n"
     ]
    }
   ],
   "source": [
    "for fe, fn in zip(files_emb, fnames):\n",
    "#     clusters_sent_4 = kmeans_sent_4.predict(fe)\n",
    "#     clusters_sent_16 = kmeans_sent_16.predict(fe)\n",
    "#     clusters_sent_64 = kmeans_sent_64.predict(fe)\n",
    "    \n",
    "    print(fn)\n",
    "    print(sorted(Counter(kmeans_doc_4.predict(fe)).items()))\n",
    "    \n",
    "#     with open(f'{fn}.clusters_doc_16.txt', 'w') as f:\n",
    "#         for item in kmeans_doc_16.predict(fe):\n",
    "#             f.write(\"%s\\n\" % item)\n",
    "    \n",
    "#     with open(f'{fn}.clusters_doc_64.txt', 'w') as f:\n",
    "#         for item in kmeans_doc_64.predict(fe):\n",
    "#             f.write(\"%s\\n\" % item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertology",
   "language": "python",
   "name": "bertology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
